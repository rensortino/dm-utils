{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(\"cuda:0\").half()\n",
    "scaling_factor = vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_path = Path(\"out/latents_01.pt\")\n",
    "embs = torch.load(emb_path, map_location=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_synth = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, emb_list in enumerate(embs):\n",
    "        if i % 20 == 0:\n",
    "            print(i)\n",
    "        emb_synth, cls = emb_list\n",
    "        emb_synth = emb_synth.to(\"cuda:0\")\n",
    "        latents_synth = (1 / scaling_factor) * emb_synth\n",
    "        latents_synth = latents_synth.half()\n",
    "        image_synth = vae.decode(latents_synth).sample\n",
    "        images_synth.append((image_synth, cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(images_synth, \"out/images_01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(images, figsize=(16,16)):\n",
    "    # Assuming you have a list of image tensors named 'image_tensors'\n",
    "    num_images = len(images)\n",
    "    num_rows = int(num_images ** 0.5)\n",
    "    num_cols = int(num_images / num_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize, tight_layout=True)\n",
    "\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i].float().cpu().numpy().transpose(1, 2, 0)\n",
    "        # image = image * 0.5 + 0.5\n",
    "        # image = image.clip(0, 1)\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensors_synth = [i[0][0].squeeze() for i in images_synth]\n",
    "print(img_tensors_synth[0].shape)\n",
    "visualize_images(random.sample(img_tensors_synth, 50), figsize=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"ImageNet10/train/n01440764\")\n",
    "image_paths = list(data_path.glob(\"*.JPEG\"))\n",
    "real_images = [TF.to_tensor(Image.open(img_path)) * 2. - 1. for img_path in image_paths]\n",
    "visualize_images(random.sample(real_images, 50), figsize=(16,16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
